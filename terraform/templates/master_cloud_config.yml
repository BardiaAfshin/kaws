#cloud-config

coreos:
  locksmith:
    endpoint: https://10.0.1.4:2379,https://10.0.1.5:2379,https://10.0.1.6:2379
    etcd_cafile: /etc/etcd2/ssl/etcd-ca.pem
    etcd_certfile: /etc/etcd2/ssl/etcd-client.pem
    etcd_keyfile: /etc/etcd2/ssl/etcd-client-key.pem
  update:
    reboot_strategy: etcd-lock
  flannel:
    etcd_endpoints: https://10.0.1.4:2379,https://10.0.1.5:2379,https://10.0.1.6:2379
    etcd_cafile: /etc/etcd2/ssl/etcd-ca.pem
    etcd_certfile: /etc/etcd2/ssl/etcd-client.pem
    etcd_keyfile: /etc/etcd2/ssl/etcd-client-key.pem
    interface: $private_ipv4
  units:
    - name: docker.service
      command: start
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
        - name: 50-opts.conf
          content: |
            [Service]
            Environment=DOCKER_OPTS='--log-driver=journald'
    - name: flanneld.service
      command: start
      drop-ins:
        - name: 20-wait-tls.conf
          content: |
            [Service]
            ExecStartPre=/opt/kaws/decrypt-pki
        - name: 50-network-config.conf
          content: |
            [Service]
            Environment=ETCD_SSL_DIR=/etc/etcd2/ssl
            Environment=ETCDCTL_CA_FILE=/etc/etcd2/ssl/etcd-ca.pem
            Environment=ETCDCTL_CERT_FILE=/etc/etcd2/ssl/etcd-client.pem
            Environment=ETCDCTL_KEY_FILE=/etc/etcd2/ssl/etcd-client-key.pem
            Environment=ETCDCTL_ENDPOINT=https://10.0.1.4:2379,https://10.0.1.5:2379,https://10.0.1.6:2379
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config "{\"Network\":\"10.2.0.0/16\"}"
    - name: kubelet.service
      command: start
      drop-ins:
        - name: 20-wait-docker.conf
          content: |
            [Unit]
            Requires=docker.service
            After=docker.service
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        [Service]
        Environment=KUBELET_IMAGE_TAG=v${version}
        Environment=KUBELET_IMAGE_URL=docker://gcr.io/google_containers/hyperkube
        Environment="RKT_RUN_ARGS=--volume resolv,kind=host,source=/etc/resolv.conf --mount volume=resolv,target=/etc/resolv.conf --insecure-options=image"
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --allow-privileged=true \
          --cloud-provider=aws \
          --cluster-dns=10.3.0.10 \
          --cluster-domain=cluster.local \
          --hostname-override=$private_ipv4 \
          --kubeconfig=/etc/kubernetes/kubeconfig.yml \
          --logtostderr=true \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --register-schedulable=false \
          --require-kubeconfig=true
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
ssh_authorized_keys: [${ssh_public_keys}]
write_files:
  - path: /etc/kubernetes/manifests/rbac.yml
    content: |
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube-proxy
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube-scheduler
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      ---
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: RoleBinding
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      subjects:
        - kind: ServiceAccount
          name: kube-controller-manager
          namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: system:kube-controller-manager
        apiGroup: rbac.authorization.k8s.io
      ---
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: RoleBinding
      metadata:
        name: kube-proxy
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      subjects:
        - kind: ServiceAccount
          name: kube-proxy
          namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: system:node-proxier
        apiGroup: rbac.authorization.k8s.io
      ---
      apiVersion: rbac.authorization.k8s.io/v1beta1
      kind: RoleBinding
      metadata:
        name: kube-scheduler
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      subjects:
        - kind: ServiceAccount
          name: kube-scheduler
          namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: system:kube-scheduler
        apiGroup: rbac.authorization.k8s.io
  - path: /etc/kubernetes/manifests/kube-apiserver.yml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      spec:
        hostNetwork: true
        containers:
          - name: kube-apiserver
            image: gcr.io/google_containers/hyperkube:v${version}
            command:
              - /hyperkube
              - apiserver
              - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds
              - --advertise-address=$private_ipv4
              - --allow-privileged=true
              - --anonymous-auth=false
              - --authorization-mode=RBAC
              - --bind-address=0.0.0.0
              - --client-ca-file=/etc/kubernetes/ssl/ca.pem
              - --cloud-provider=aws
              - --etcd-cafile=/etc/etcd2/ssl/etcd-ca.pem
              - --etcd-certfile=/etc/etcd2/ssl/etcd-client.pem
              - --etcd-keyfile=/etc/etcd2/ssl/etcd-client-key.pem
              - --etcd-servers=https://10.0.1.4:2379,https://10.0.1.5:2379,https://10.0.1.6:2379
              - --external-hostname=https://kubernetes.${domain}
              - --insecure-bind-address=0.0.0.0
              - --runtime-config=batch/v2alpha1=true
              - --secure-port=443
              - --service-account-key-file=/etc/kubernetes/ssl/master-key.pem
              - --service-cluster-ip-range=10.3.0.1/24
              - --storage-backend=etcd2
              - --storage-media-type=application/json
              - --tls-cert-file=/etc/kubernetes/ssl/master.pem
              - --tls-private-key-file=/etc/kubernetes/ssl/master-key.pem
            ports:
              - containerPort: 443
                hostPort: 443
                name: https
              - containerPort: 8080
                hostPort: 8080
                name: local
            volumeMounts:
              - mountPath: /etc/etcd2/ssl
                name: ssl-certs-etcd
                readOnly: true
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
        volumes:
          - hostPath:
              path: /etc/etcd2/ssl
            name: ssl-certs-etcd
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
  - path: /etc/kubernetes/manifests/kube-proxy.yml
    content: |
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-proxy
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      data:
        kubeconfig.yml: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: https://${master_ip}:443
              name: local
          contexts:
            - context:
                cluster: local
                user: local
              name: local
          current-context: local
          users:
            - name: local
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      ---
      apiVersion: extensions/v1beta1
      kind: DaemonSet
      metadata:
        name: kube-proxy
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      spec:
        selector:
          matchLabels:
            kaws-app: kube-proxy
        template:
          metadata:
            labels:
              kaws-app: kube-proxy
            annotations:
              kaws.inquicker.com/managed: "true"
          spec:
            hostNetwork: true
            serviceAccountName: kube-proxy
            containers:
              - name: kube-proxy
                image: gcr.io/google_containers/hyperkube:v${version}
                command:
                  - /hyperkube
                  - proxy
                  - --healthz-bind-address=0.0.0.0
                  - --kubeconfig=/etc/kubernetes/kubeconfig.yml
                  - --proxy-mode=iptables
                securityContext:
                  privileged: true
                volumeMounts:
                  - mountPath: /etc/ssl/certs
                    name: ssl-certs-host
                    readOnly: true
                  - mountPath: /etc/kubernetes
                    name: kubeconfig
                    readOnly: true
            volumes:
              - hostPath:
                  path: /usr/share/ca-certificates
                name: ssl-certs-host
              - configMap:
                  name: kube-proxy
                name: kubeconfig
  - path: /etc/kubernetes/manifests/kube-controller-manager.yml
    content: |
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      data:
        kubeconfig.yml: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: http://127.0.0.1:8080
              name: local
          contexts:
            - context:
                cluster: local
                user: local
              name: local
          current-context: local
          users:
            - name: local
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      ---
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      spec:
        hostNetwork: true
        serviceAccountName: kube-controller-manager
        containers:
          - name: kube-controller-manager
            image: gcr.io/google_containers/hyperkube:v${version}
            command:
              - /hyperkube
              - controller-manager
              - --cloud-provider=aws
              - --kubeconfig=/etc/kubernetes/config/kubeconfig.yml
              - --leader-elect=true
              - --root-ca-file=/etc/kubernetes/ssl/ca.pem
              - --service-account-private-key-file=/etc/kubernetes/ssl/master-key.pem
              - --use-service-account-credentials=true
            resources:
              requests:
                cpu: 200m
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 1
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
              - mountPath: /etc/kubernetes/config
                name: kubeconfig
                readOnly: true
        volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - configMap:
              name: kube-proxy
            name: kubeconfig
  - path: /etc/kubernetes/manifests/kube-dns.yml
    content: |
      ---
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: kube-dns
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      ---
      apiVersion: v1
      kind: Service
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
          kubernetes.io/cluster-service: "true"
          kubernetes.io/name: "KubeDNS"
        annotations:
          kaws.inquicker.com/managed: "true"
      spec:
        selector:
          k8s-app: kube-dns
        clusterIP: 10.3.0.10
        ports:
          - name: dns
            port: 53
            protocol: UDP
          - name: dns-tcp
            port: 53
            protocol: TCP
      ---
      apiVersion: apps/v1beta1
      kind: Deployment
      metadata:
        name: kube-dns
        namespace: kube-system
        labels:
          k8s-app: kube-dns
        annotations:
          kaws.inquicker.com/managed: "true"
      spec:
        strategy:
          rollingUpdate:
            maxSurge: 10%
            maxUnavailable: 0
        selector:
          matchLabels:
            k8s-app: kube-dns
        template:
          metadata:
            labels:
              k8s-app: kube-dns
            annotations:
              scheduler.alpha.kubernetes.io/critical-pod: ""
              kaws.inquicker.com/managed: "true"
          spec:
            containers:
              - name: kubedns
                image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
                imagePullPolicy: IfNotPresent
                resources:
                  limits:
                    memory: 170Mi
                  requests:
                    cpu: 100m
                    memory: 70Mi
                livenessProbe:
                  httpGet:
                    path: /healthcheck/kubedns
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 8081
                    scheme: HTTP
                  initialDelaySeconds: 3
                  timeoutSeconds: 5
                args:
                  - --domain=cluster.local.
                  - --dns-port=10053
                  - --config-dir=/kube-dns-config
                env:
                  - name: PROMETHEUS_PORT
                    value: "10055"
                ports:
                  - containerPort: 10053
                    name: dns-local
                    protocol: UDP
                  - containerPort: 10053
                    name: dns-tcp-local
                    protocol: TCP
                  - containerPort: 10055
                    name: metrics
                    protocol: TCP
                volumeMounts:
                  - name: kube-dns-config
                    mountPath: /kube-dns-config
              - name: dnsmasq
                image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
                imagePullPolicy: IfNotPresent
                livenessProbe:
                  httpGet:
                    path: /healthcheck/dnsmasq
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                  - -configDir=/etc/k8s/dns/dnsmasq-nanny
                  - -restartDnsmasq=true
                  - --
                  - -k
                  - --cache-size=1000
                  - --log-facility=-
                  - --server=/cluster.local/127.0.0.1#10053
                  - --server=/in-addr.arpa/127.0.0.1#10053
                  - --server=/ip6.arpa/127.0.0.1#10053
                ports:
                  - containerPort: 53
                    name: dns
                    protocol: UDP
                  - containerPort: 53
                    name: dns-tcp
                    protocol: TCP
                resources:
                  requests:
                    cpu: 150m
                    memory: 20Mi
                volumeMounts:
                  - name: kube-dns-config
                    mountPath: /etc/k8s/dns/dnsmasq-nanny
              - name: sidecar
                image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
                imagePullPolicy: IfNotPresent
                livenessProbe:
                  httpGet:
                    path: /metrics
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                  - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
                  - --probe-dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
                ports:
                  - containerPort: 10054
                    name: metrics
                    protocol: TCP
                resources:
                  requests:
                    memory: 20Mi
                    cpu: 10m
            dnsPolicy: Default
            serviceAccountName: kube-dns
            volumes:
              - name: kube-dns-config
                configMap:
                  name: kube-dns
                  optional: true
  - path: /etc/kubernetes/manifests/kube-scheduler.yml
    content: |
      ---
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-scheduler
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      data:
        kubeconfig.yml: |
          apiVersion: v1
          kind: Config
          clusters:
            - cluster:
                certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                server: http://127.0.0.1:8080
              name: local
          contexts:
            - context:
                cluster: local
                user: local
              name: local
          current-context: local
          users:
            - name: local
              user:
                tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      ---
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
        annotations:
          kaws.inquicker.com/managed: "true"
      spec:
        hostNetwork: true
        serviceAccountName: kube-scheduler
        containers:
          - name: kube-scheduler
            image: gcr.io/google_containers/hyperkube:v${version}
            command:
              - /hyperkube
              - scheduler
              - --kubeconfig=/etc/kubernetes/kubeconfig.yml
              - --leader-elect=true
            resources:
              requests:
                cpu: 100m
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 15
            volumeMounts:
              - mountPath: /etc/kubernetes
                name: kubeconfig
                readOnly: true
        volumes:
          - configMap:
              name: kube-proxy
            name: kubeconfig
  - path: /etc/kubernetes/kubeconfig.yml
    content: |
      apiVersion: v1
      kind: Config
      clusters:
        - name: local
          cluster:
            server: http://127.0.0.1:8080
      contexts:
        - context:
            cluster: local
            user: local
          name: local
      current-context: local
      users:
        - name: local
  - path: /opt/kaws/decrypt-pki
    permissions: "0500"
    content: |
      #!/bin/bash -e
      for file in $(find /etc/etcd2/ssl/*.binary /etc/kubernetes/ssl/*.binary); do
        /usr/bin/rkt run \
          --net=host \
          --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
          --mount=volume=dns,target=/etc/resolv.conf  \
          --volume=etcd-pki,kind=host,source=/etc/etcd2/ssl \
          --mount=volume=etcd-pki,target=/etc/etcd2/ssl \
          --volume=k8s-pki,kind=host,source=/etc/kubernetes/ssl \
          --mount=volume=k8s-pki,target=/etc/kubernetes/ssl \
          --trust-keys-from-https \
           quay.io/coreos/awscli \
           --exec=/bin/bash \
           -- \
           -c "aws --region ${region} kms decrypt --ciphertext-blob fileb://$file --output text --query Plaintext | base64 -d > $${file/-encrypted.binary/.pem}"
      done
  - path: /etc/etcd2/ssl/etcd-ca.pem
    encoding: "base64"
    content: "${etcd_ca_cert}"
  - path: /etc/etcd2/ssl/etcd-client.pem
    encoding: "base64"
    content: "${etcd_client_cert}"
  - path: /etc/etcd2/ssl/etcd-client-key-encrypted.binary
    encoding: "base64"
    content: "${etcd_client_key}"
  - path: /etc/kubernetes/ssl/ca.pem
    encoding: "base64"
    content: "${k8s_ca_cert}"
  - path: /etc/kubernetes/ssl/master.pem
    encoding: "base64"
    content: "${k8s_master_cert}"
  - path: /etc/kubernetes/ssl/master-key-encrypted.binary
    encoding: "base64"
    content: "${k8s_master_key}"
